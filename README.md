# Titanic - Machine Learning from Disaster

Welcome to my Titanic Machine Learning project!  
This project is based on the classic Kaggle competition: [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic).

The goal is to build a machine learning model that predicts whether a passenger survived the Titanic shipwreck based on features like age, sex, and class.

---

## Project Structure 
```bash
Titanic-ML/
â”œâ”€â”€ data/              # Raw datasets (train.csv, test.csv)
â”œâ”€â”€ notebooks/         # Jupyter notebooks
â”œâ”€â”€ output/            # Submission files
â”œâ”€â”€ models/            # (Optional) Saved models
â”œâ”€â”€ venv/              # Python virtual environment
â”œâ”€â”€ .gitignore         # Files to be ignored by Git
â”œâ”€â”€ README.md          # Project overview (this file)
â””â”€â”€ requirements.txt   # Required Python packages
```

---

## Approach ğŸ§ 

- Selected simple features: `Pclass`, `Sex`, `Age`,`fare`
- Preprocessing steps:
  - Converted `Sex` to numeric values (male â†’ 0, female â†’ 1)
  - Filled missing `Age` values with median
- Built and trained a Logistic Regression model using scikit-learn
- Predicted survival on the test set
- Created a Kaggle submission file



## Results ğŸ¯

- **Validation Accuracy:** ~74.8%
- **Kaggle Public Leaderboard Score:** 0.75837



## How to Run ğŸ’»
1. Clone this repository:

   ```bash
   git clone git@github.com:lesa-hettiarachchi/Titanic-ML.git
   cd Titanic-ML
   ```
2. Create and activate a virtual environment:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Mac/Linux
   venv\Scripts\activate     # Windows
   ```
3. Install required packages:

  	```bash
   pip install -r requirements.txt
   ```
4. Run the Jupyter notebooks:

   ```bash
   jupyter notebook
   ```
   
5.	Execute the notebook cells to train the model and generate submission.csv
  
6.	Upload submission.csv to Kaggle for evaluation!



## Future Work ğŸš€

- Add more features (Embarked, Siblings or spouses, Parents or childern)

- Handle missing data more intelligently

- Try different models (Random Forest, XGBoost)

- Hyperparameter tuning using GridSearchCV

- Apply cross-validation for better evaluation


# Acknowledgements ğŸ“

- Kaggle Titanic Competition

- scikit-learn documentation

- pandas and numpy libraries

   
